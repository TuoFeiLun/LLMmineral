#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„åœ°è´¨æ–‡æœ¬RAGç³»ç»Ÿ - åªå¤„ç†å¯¹æ–‡æœ¬åˆ†ææœ‰ä»·å€¼çš„æ–‡ä»¶
ä¸“é—¨é’ˆå¯¹åœ°çƒç§‘å­¦LLMé¡¹ç›®è®¾è®¡
"""
import sys
from pathlib import Path

# æ·»åŠ ragæ¨¡å—åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / "rag"))

from geological_text_processor import GeologicalTextProcessor

def main():
    print("ğŸŒ ä¼˜åŒ–çš„åœ°è´¨æ–‡æœ¬RAGç³»ç»Ÿ")
    print("ä¸“é—¨é’ˆå¯¹åœ°çƒç§‘å­¦LLMé¡¹ç›®ä¼˜åŒ–")
    print("="*50)
    
    # ä½ çš„æ•°æ®è·¯å¾„
    data_path = "/Users/yjli/QUTIT/semester4/ifn712/datacollect/from_orefox"
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = GeologicalTextProcessor(enable_ocr=True)
    
    # 1. é¦–å…ˆåˆ†æä½ çš„æ•°æ®
    print("ğŸ” åˆ†æä½ çš„åœ°è´¨æ•°æ®...")
    try:
        analysis = processor.analyze_data_directory(data_path)
        
        print(f"\nğŸ“Š æ•°æ®åˆ†æç»“æœ:")
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {analysis['total_files']}")
        
        print(f"\nâœ… é«˜ä¼˜å…ˆçº§æ–‡ä»¶ï¼ˆå»ºè®®å¤„ç†ï¼‰:")
        for ext, count in analysis['high_priority'].items():
            print(f"  {ext}: {count} ä¸ª")
        
        print(f"\nâš ï¸  ä¸­ç­‰ä¼˜å…ˆçº§æ–‡ä»¶ï¼ˆå¯é€‰å¤„ç†ï¼‰:")
        for ext, count in analysis['medium_priority'].items():
            print(f"  {ext}: {count} ä¸ª")
            
        print(f"\nâ­ï¸  å»ºè®®è·³è¿‡çš„æ–‡ä»¶:")
        skip_total = sum(analysis['skip'].values())
        print(f"  æ€»è®¡: {skip_total} ä¸ªæ–‡ä»¶")
        
        print(f"\nğŸ’¡ å¤„ç†å»ºè®®:")
        for rec in analysis['recommendations']:
            print(f"  {rec}")
            
    except Exception as e:
        print(f"âŒ æ•°æ®åˆ†æå¤±è´¥: {e}")
        return
    
    # 2. è¯¢é—®ç”¨æˆ·å¤„ç†ç­–ç•¥
    print(f"\nğŸ¯ å¤„ç†ç­–ç•¥é€‰æ‹©:")
    print("1. ä»…å¤„ç†é«˜ä¼˜å…ˆçº§æ–‡ä»¶ï¼ˆPDFã€æ–‡æœ¬ã€æ•°æ®æ–‡ä»¶ï¼‰- æ¨è")
    print("2. åŒ…å«TIFæ–‡ä»¶å¤„ç†ï¼ˆå¯èƒ½åŒ…å«æ‰«ææ–‡æ¡£ï¼‰")
    print("3. ä»…åˆ†æï¼Œä¸å¤„ç†æ•°æ®")
    
    choice = input("è¯·é€‰æ‹© (1-3): ").strip()
    
    if choice == '3':
        print("ğŸ‘‹ åˆ†æå®Œæˆï¼Œé€€å‡º")
        return
    
    include_tif = choice == '2'
    
    # 3. å¤„ç†æ•°æ®
    print(f"\nğŸš€ å¼€å§‹å¤„ç†æ•°æ®...")
    if include_tif:
        print("âš ï¸  åŒ…å«TIFæ–‡ä»¶å¤„ç†ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
    else:
        print("âœ… ä»…å¤„ç†æ–‡æœ¬ç›¸å…³æ–‡ä»¶ï¼Œé€Ÿåº¦è¾ƒå¿«")
    
    try:
        rag = processor.process_geological_data(
            directory_path=data_path,
            process_medium_priority=include_tif,
            model_name="qwen2.5:7b"
        )
        
        print(f"\nğŸ‰ æ•°æ®å¤„ç†å®Œæˆï¼")
        
        # 4. æµ‹è¯•æŸ¥è¯¢
        print(f"\nğŸ§ª æµ‹è¯•åœ°è´¨æ–‡æœ¬æŸ¥è¯¢åŠŸèƒ½...")
        
        test_queries = [
            "è¿™äº›æ•°æ®ä¸­åŒ…å«å“ªäº›ç±»å‹çš„åœ°è´¨ä¿¡æ¯ï¼Ÿ",
            "æœ‰å¤šå°‘ä¸ªPDFæ–‡æ¡£ï¼Ÿ",
            "æ•°æ®ä¸­åŒ…å«å“ªäº›åœ°è´¨è°ƒæŸ¥æŠ¥å‘Šï¼Ÿ",
            "è¿™äº›æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
        
        for i, query in enumerate(test_queries, 1):
            print(f"\næŸ¥è¯¢ {i}: {query}")
            result = rag.query(query)
            print(f"å›ç­”: {result['answer'][:300]}...")
            
            # æ˜¾ç¤ºç›¸å…³æ–‡æ¡£
            sources = result.get('sources', [])
            if sources:
                print(f"ç›¸å…³æ–‡æ¡£ ({len(sources)}ä¸ª):")
                for j, source in enumerate(sources[:2], 1):
                    metadata = source.get('metadata', {})
                    file_name = metadata.get('file_name', 'Unknown')
                    file_type = metadata.get('file_type', 'unknown')
                    print(f"  {j}. {file_name} ({file_type})")
        
        # 5. äº¤äº’å¼æŸ¥è¯¢
        print(f"\nğŸ¤– äº¤äº’å¼æŸ¥è¯¢æ¨¡å¼")
        print("ç°åœ¨ä½ å¯ä»¥æŸ¥è¯¢ä½ çš„åœ°è´¨æ–‡æœ¬æ•°æ®äº†ï¼")
        print("è¾“å…¥é—®é¢˜ï¼Œè¾“å…¥ 'quit' é€€å‡º")
        
        while True:
            try:
                user_query = input("\nğŸ” è¯·è¾“å…¥é—®é¢˜: ").strip()
                
                if user_query.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                if not user_query:
                    continue
                
                result = rag.query(user_query)
                print(f"\nğŸ’¡ å›ç­”: {result['answer']}")
                
                # æ˜¾ç¤ºç›¸å…³æ–‡æ¡£
                sources = result.get('sources', [])
                if sources:
                    print(f"\nğŸ“š ç›¸å…³æ–‡æ¡£:")
                    for i, source in enumerate(sources[:3], 1):
                        metadata = source.get('metadata', {})
                        file_name = metadata.get('file_name', 'Unknown')
                        file_type = metadata.get('file_type', 'unknown')
                        score = source.get('score', 0)
                        print(f"  {i}. {file_name} ({file_type}) - ç›¸ä¼¼åº¦: {score:.3f}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
                
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        print("\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. ç¡®ä¿OllamaæœåŠ¡è¿è¡Œ: ollama serve")
        print("2. ç¡®ä¿æ¨¡å‹å·²ä¸‹è½½: ollama pull qwen2.5:7b")
        print("3. æ£€æŸ¥æ•°æ®è·¯å¾„æ˜¯å¦æ­£ç¡®")

def quick_analysis():
    """å¿«é€Ÿåˆ†ææ¨¡å¼ - ä¸å¤„ç†æ•°æ®ï¼Œä»…åˆ†æ"""
    print("ğŸ” å¿«é€Ÿæ•°æ®åˆ†ææ¨¡å¼")
    print("="*30)
    
    data_path = "/Users/yjli/QUTIT/semester4/ifn712/datacollect/from_orefox"
    processor = GeologicalTextProcessor()
    
    try:
        analysis = processor.analyze_data_directory(data_path)
        
        print(f"ğŸ“Š ä½ çš„æ•°æ®ç»Ÿè®¡:")
        print(f"æ€»æ–‡ä»¶: {analysis['total_files']}")
        
        print(f"\nâœ… å»ºè®®å¤„ç†çš„æ–‡ä»¶:")
        high_total = sum(analysis['high_priority'].values())
        print(f"é«˜ä¼˜å…ˆçº§: {high_total} ä¸ª")
        for ext, count in sorted(analysis['high_priority'].items()):
            print(f"  {ext}: {count}")
            
        print(f"\nâš ï¸  å¯é€‰å¤„ç†çš„æ–‡ä»¶:")
        medium_total = sum(analysis['medium_priority'].values())  
        print(f"ä¸­ç­‰ä¼˜å…ˆçº§: {medium_total} ä¸ª")
        for ext, count in sorted(analysis['medium_priority'].items()):
            print(f"  {ext}: {count}")
        
        print(f"\nâ­ï¸  å»ºè®®è·³è¿‡çš„æ–‡ä»¶:")
        skip_total = sum(analysis['skip'].values())
        print(f"è·³è¿‡: {skip_total} ä¸ª")
        
        print(f"\nğŸ’¾ å­˜å‚¨ä¼˜åŒ–:")
        process_ratio = (high_total + medium_total) / analysis['total_files'] * 100
        print(f"å»ºè®®å¤„ç†: {process_ratio:.1f}% çš„æ–‡ä»¶")
        print(f"å¯èŠ‚çœ: {100-process_ratio:.1f}% çš„å¤„ç†æ—¶é—´")
        
        print(f"\nğŸ“‹ å»ºè®®:")
        for rec in analysis['recommendations']:
            print(f"  {rec}")
            
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ä¼˜åŒ–çš„åœ°è´¨æ–‡æœ¬RAGç³»ç»Ÿ")
    parser.add_argument("--analyze-only", action="store_true", help="ä»…åˆ†ææ•°æ®ï¼Œä¸å¤„ç†")
    
    args = parser.parse_args()
    
    if args.analyze_only:
        quick_analysis()
    else:
        main()
