from llama_index.core import VectorStoreIndex, Settings, StorageContext, Document
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb
import os
import argparse
import sys
from pathlib import Path

# è®¾ç½®è·¯å¾„
sys.path.append(str(Path(__file__).parent / "rag"))

def setup_models(llm_model="qwen2.5:7b", embed_model_name="nomic-embed-text"):
    """è®¾ç½®LLMå’ŒåµŒå…¥æ¨¡å‹"""
    print("ğŸš€ è®¾ç½®æ¨¡å‹...")
    llm = Ollama(model=llm_model, base_url="http://localhost:11434", request_timeout=300.0)
    embed_model = OllamaEmbedding(model_name=embed_model_name, base_url="http://localhost:11434")
    
    Settings.llm = llm
    Settings.embed_model = embed_model
    
    # è®¾ç½®æ–‡æ¡£åˆ†å—å‚æ•°
    Settings.chunk_size = 1024
    Settings.chunk_overlap = 50
    
    print(f"âœ… æ¨¡å‹è®¾ç½®å®Œæˆ: LLM={llm_model}, åµŒå…¥={embed_model_name}")
    return llm, embed_model

def load_documents(data_path):
    """åŠ è½½æ–‡æ¡£"""
    from llama_index.core.readers import SimpleDirectoryReader
    from llama_index.readers.file import PDFReader
    from llama_index.core.readers.json import JSONReader
    
    print(f"ğŸ“ å¤„ç†ç›®å½•: {data_path}")
      # ä¸“ç”¨å¤„ç†å™¨
    pdf_reader = PDFReader()
    json_reader = JSONReader()
    
    reader = SimpleDirectoryReader(
        input_dir=data_path,
        file_extractor={
            ".pdf": pdf_reader,
            ".docx": "default", 
            ".txt": "default",
            ".json": json_reader
        },
        recursive=True
    )
    
    documents = reader.load_data()
    print(f"âœ… åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    return documents

def add_documents_to_collection(data_path, db_path, collection_name="documents", update_mode="append"):
    """
    å‘ç°æœ‰é›†åˆæ·»åŠ æˆ–æ›´æ–°æ–‡æ¡£
    
    Args:
        data_path: æ–°æ–‡æ¡£è·¯å¾„
        db_path: æ•°æ®åº“è·¯å¾„  
        collection_name: é›†åˆåç§°
        update_mode: æ›´æ–°æ¨¡å¼ ("append"=è¿½åŠ , "replace"=æ›¿æ¢, "merge"=æ™ºèƒ½åˆå¹¶)
    """
    print(f"ğŸ“ å‘é›†åˆ {collection_name} æ·»åŠ æ–‡æ¡£ (æ¨¡å¼: {update_mode})...")
    
    # åˆ›å»ºæ•°æ®åº“ç›®å½•
    os.makedirs(db_path, exist_ok=True)
    chroma_client = chromadb.PersistentClient(path=db_path)
    
    # è·å–æˆ–åˆ›å»ºé›†åˆ
    try:
        chroma_collection = chroma_client.get_collection(collection_name)
        print(f"âœ… æ‰¾åˆ°ç°æœ‰é›†åˆ {collection_name}, å½“å‰åŒ…å« {chroma_collection.count()} ä¸ªæ–‡æ¡£")
        collection_exists = True
    except:
        chroma_collection = chroma_client.create_collection(collection_name)
        print(f"ğŸ†• åˆ›å»ºæ–°é›†åˆ {collection_name}")
        collection_exists = False
    
    # åŠ è½½æ–°æ–‡æ¡£
    new_documents = load_documents(data_path)
    if not new_documents:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–°æ–‡æ¡£")
        return None
    
    print(f"ğŸ“„ å‡†å¤‡æ·»åŠ  {len(new_documents)} ä¸ªæ–°æ–‡æ¡£")
    
    # æ ¹æ®æ›´æ–°æ¨¡å¼å¤„ç†
    if update_mode == "replace" or not collection_exists:
        # æ›¿æ¢æ¨¡å¼ï¼šæ¸…ç©ºç°æœ‰æ•°æ®
        if collection_exists and chroma_collection.count() > 0:
            print("ğŸ—‘ï¸  æ¸…ç©ºç°æœ‰é›†åˆæ•°æ®...")
            chroma_client.delete_collection(collection_name)
            chroma_collection = chroma_client.create_collection(collection_name)
        
        # åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨å’Œç´¢å¼•
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        
        print("ğŸ§® ç”Ÿæˆå‘é‡ç´¢å¼•...")
        index = VectorStoreIndex.from_documents(
            new_documents,
            storage_context=storage_context,
            show_progress=True
        )
        
    elif update_mode == "append":
        # è¿½åŠ æ¨¡å¼ï¼šç›´æ¥æ·»åŠ æ–°æ–‡æ¡£
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        
        # å¦‚æœé›†åˆä¸ºç©ºï¼Œåˆ›å»ºæ–°ç´¢å¼•
        if chroma_collection.count() == 0:
            print("ğŸ§® åˆ›å»ºæ–°çš„å‘é‡ç´¢å¼•...")
            index = VectorStoreIndex.from_documents(
                new_documents,
                storage_context=storage_context,
                show_progress=True
            )
        else:
            # åŠ è½½ç°æœ‰ç´¢å¼•å¹¶æ·»åŠ æ–°æ–‡æ¡£
            print("ğŸ“š åŠ è½½ç°æœ‰ç´¢å¼•...")
            index = VectorStoreIndex.from_vector_store(vector_store)
            
            print("â• å‘ç°æœ‰ç´¢å¼•æ·»åŠ æ–°æ–‡æ¡£...")
            for doc in new_documents:
                index.insert(doc)
            
    elif update_mode == "merge":
        # æ™ºèƒ½åˆå¹¶æ¨¡å¼ï¼šæ£€æŸ¥é‡å¤å¹¶åˆå¹¶
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        
        if chroma_collection.count() == 0:
            # ç©ºé›†åˆï¼Œç›´æ¥åˆ›å»º
            index = VectorStoreIndex.from_documents(
                new_documents,
                storage_context=storage_context,
                show_progress=True
            )
        else:
            # åŠ è½½ç°æœ‰ç´¢å¼•
            index = VectorStoreIndex.from_vector_store(vector_store)
            
            # æ£€æŸ¥å¹¶æ·»åŠ éé‡å¤æ–‡æ¡£
            print("ğŸ” æ£€æŸ¥æ–‡æ¡£é‡å¤æ€§...")
            added_count = 0
            for doc in new_documents:
                # ç®€å•çš„é‡å¤æ£€æŸ¥ï¼ˆåŸºäºæ–‡æ¡£å†…å®¹å“ˆå¸Œæˆ–æ–‡ä»¶åï¼‰
                doc_id = getattr(doc, 'doc_id', None) or hash(doc.text[:100])
                
                # å°è¯•æŸ¥è¯¢æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼æ–‡æ¡£
                try:
                    existing_docs = index.as_retriever(similarity_top_k=1).retrieve(doc.text[:200])
                    if existing_docs and len(existing_docs) > 0:
                        # å¦‚æœç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œè·³è¿‡
                        similarity = getattr(existing_docs[0], 'score', 0)
                        if similarity > 0.95:  # å¯è°ƒæ•´é˜ˆå€¼
                            continue
                except:
                    pass
                
                index.insert(doc)
                added_count += 1
            
            print(f"âœ… æ·»åŠ äº† {added_count} ä¸ªæ–°æ–‡æ¡£ï¼ˆè·³è¿‡ {len(new_documents) - added_count} ä¸ªé‡å¤æ–‡æ¡£ï¼‰")
    
    # éªŒè¯æœ€ç»ˆç»“æœ
    final_count = chroma_collection.count()
    print(f"âœ… é›†åˆ {collection_name} ç°åœ¨åŒ…å« {final_count} ä¸ªå‘é‡")
    
    return index

def batch_add_documents(data_paths, db_path, collection_name="documents", update_mode="append"):
    """
    æ‰¹é‡æ·»åŠ å¤šä¸ªæ•°æ®è·¯å¾„çš„æ–‡æ¡£åˆ°åŒä¸€é›†åˆ
    
    Args:
        data_paths: æ•°æ®è·¯å¾„åˆ—è¡¨
        db_path: æ•°æ®åº“è·¯å¾„
        collection_name: é›†åˆåç§°
        update_mode: æ›´æ–°æ¨¡å¼
    """
    print(f"ğŸ“¦ æ‰¹é‡æ·»åŠ æ–‡æ¡£åˆ°é›†åˆ {collection_name}")
    
    index = None
    for i, data_path in enumerate(data_paths):
        print(f"\nğŸ”„ å¤„ç†ç¬¬ {i+1}/{len(data_paths)} ä¸ªæ•°æ®è·¯å¾„: {data_path}")
        
        # ç¬¬ä¸€æ¬¡åˆ›å»ºï¼Œåç»­éƒ½æ˜¯è¿½åŠ 
        mode = update_mode if i == 0 else "append"
        index = add_documents_to_collection(data_path, db_path, collection_name, mode)
        
        if index is None:
            print(f"âŒ å¤„ç†è·¯å¾„ {data_path} å¤±è´¥")
            continue
    
    return index
def load_existing_database(db_path):
    """åŠ è½½ç°æœ‰æ•°æ®åº“"""
    print("ğŸ’¾ åŠ è½½ç°æœ‰æ•°æ®åº“...")
    
    if not os.path.exists(db_path):
        print("âŒ æ•°æ®åº“ä¸å­˜åœ¨")
        return None
    
    try:
        chroma_client = chromadb.PersistentClient(path=db_path)
        chroma_collection = chroma_client.get_collection("documents")
        existing_count = chroma_collection.count()
        
        if existing_count == 0:
            print("âš ï¸  æ•°æ®åº“ä¸ºç©º")
            return None
        
        print(f"âœ… åŠ è½½ç°æœ‰æ•°æ®åº“ï¼ŒåŒ…å« {existing_count} ä¸ªå‘é‡")
        
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        index = VectorStoreIndex.from_vector_store(vector_store)
        
        return index
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®åº“å¤±è´¥: {e}")
        return None

def list_collection_info(db_path, collection_name=None):
    """åˆ—å‡ºé›†åˆä¿¡æ¯"""
    chroma_client = chromadb.PersistentClient(path=db_path)
    
    if collection_name:
        # æ˜¾ç¤ºç‰¹å®šé›†åˆä¿¡æ¯
        try:
            collection = chroma_client.get_collection(collection_name)
            count = collection.count()
            print(f"ğŸ“Š é›†åˆ '{collection_name}' åŒ…å« {count} ä¸ªæ–‡æ¡£")
            
            # è·å–ä¸€äº›æ ·æœ¬æ–‡æ¡£ä¿¡æ¯
            if count > 0:
                sample = collection.peek(limit=3)
                print("ğŸ“ æ ·æœ¬æ–‡æ¡£:")
                for i, doc in enumerate(sample.get('documents', [])[:3]):
                    print(f"  {i+1}. {doc[:100]}...")
                    
        except Exception as e:
            print(f"âŒ é›†åˆ '{collection_name}' ä¸å­˜åœ¨æˆ–è®¿é—®å¤±è´¥: {e}")
    else:
        # æ˜¾ç¤ºæ‰€æœ‰é›†åˆ
        collections = chroma_client.list_collections()
        print(f"ğŸ“‹ æ•°æ®åº“åŒ…å« {len(collections)} ä¸ªé›†åˆ:")
        for col in collections:
            count = col.count()
            print(f"  - {col.name}: {count} ä¸ªæ–‡æ¡£")

def test_queries(index, queries=None):
    """æµ‹è¯•æŸ¥è¯¢"""
    if queries is None:
        queries = [
             
            
            "give some information about the VL-4PQ Base",
            "give some information about volcanics",            
        ]
    
    print("ğŸ§ª æµ‹è¯•æŸ¥è¯¢...")
    query_engine = index.as_query_engine()
    
    for query in queries:
        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        try:
            response = query_engine.query(query)
            print(f"ğŸ’¡ å›ç­”: {response}")
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            print("ğŸ’¡ å¯èƒ½çš„åŸå› : LLMå“åº”è¶…æ—¶æˆ–OllamaæœåŠ¡é—®é¢˜")


# ä¿®æ”¹mainå‡½æ•°ä»¥æ”¯æŒæ–°çš„åŠŸèƒ½
def main():
    parser = argparse.ArgumentParser(description="åœ°è´¨æ•°æ®RAGç³»ç»Ÿ - æ”¯æŒå¢é‡æ›´æ–°")
    parser.add_argument("--mode", 
                        choices=["create", "load", "auto", "add", "batch-add", "info"], 
                        default="auto",
                        help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--update-mode", 
                        choices=["append", "replace", "merge"], 
                        default="append",
                        help="æ›´æ–°æ¨¡å¼: append=è¿½åŠ , replace=æ›¿æ¢, merge=æ™ºèƒ½åˆå¹¶")
    parser.add_argument("--data-paths", nargs="*", 
                        help="å¤šä¸ªæ•°æ®è·¯å¾„ï¼ˆç”¨äºbatch-addæ¨¡å¼ï¼‰")
    parser.add_argument("--collection-name", default="documents", 
                        help="é›†åˆåç§°")
    # ... å…¶ä»–ç°æœ‰å‚æ•° ...
    
    args = parser.parse_args()
    
    print("ğŸŒ åœ°è´¨æ•°æ®RAGç³»ç»Ÿ - å¢é‡æ›´æ–°ç‰ˆ")
    print("="*50)
    print(f"ğŸ“‹ è¿è¡Œæ¨¡å¼: {args.mode}")
    print(f"ğŸ“ æ•°æ®è·¯å¾„: {args.data_path}")
    print(f"ğŸ’¾ æ•°æ®åº“è·¯å¾„: {args.db_path}")
    print(f"ğŸ“š é›†åˆåç§°: {args.collection_name}")
    if args.mode in ["add", "batch-add"]:
        print(f"ğŸ”„ æ›´æ–°æ¨¡å¼: {args.update_mode}")
    print("="*50)
    
    # è®¾ç½®æ¨¡å‹
    setup_models(args.llm_model, args.embed_model)
    
    index = None
    
    if args.mode == "add":
        # æ·»åŠ æ–‡æ¡£åˆ°ç°æœ‰é›†åˆ
        index = add_documents_to_collection(
            args.data_path, 
            args.db_path, 
            args.collection_name,
            args.update_mode
        )
        
    elif args.mode == "batch-add":
        # æ‰¹é‡æ·»åŠ å¤šä¸ªè·¯å¾„çš„æ–‡æ¡£
        data_paths = args.data_paths or [args.data_path]
        index = batch_add_documents(
            data_paths,
            args.db_path,
            args.collection_name,
            args.update_mode
        )
        
    elif args.mode == "info":
        # æ˜¾ç¤ºé›†åˆä¿¡æ¯
        list_collection_info(args.db_path, args.collection_name)
        return
        
    # ... åŸæœ‰çš„å…¶ä»–æ¨¡å¼å¤„ç† ...
    
    if index and args.mode != "info":
        # æµ‹è¯•æŸ¥è¯¢
        test_queries(index)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        list_collection_info(args.db_path, args.collection_name)
    
    print(f"\nâœ… å®Œæˆï¼æ•°æ®åº“ä½ç½®: {args.db_path}")

if __name__ == "__main__":
    setup_models()
    db_path = "./simple_geological_db"
    index = load_existing_database(db_path)
    test_queries(index)