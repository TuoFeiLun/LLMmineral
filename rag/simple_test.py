#!/usr/bin/env python3
import sys
from pathlib import Path
import os
import argparse

# è®¾ç½®è·¯å¾„
sys.path.append(str(Path(__file__).parent / "rag"))

# ç›´æ¥å¯¼å…¥å¹¶ä½¿ç”¨
from llama_index.core import VectorStoreIndex, Settings, StorageContext, Document
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

def setup_models(llm_model="qwen2.5:7b", embed_model_name="nomic-embed-text"):
    """è®¾ç½®LLMå’ŒåµŒå…¥æ¨¡å‹"""
    print("ğŸš€ è®¾ç½®æ¨¡å‹...")
    llm = Ollama(model=llm_model, base_url="http://localhost:11434", request_timeout=300.0)
    embed_model = OllamaEmbedding(model_name=embed_model_name, base_url="http://localhost:11434")
    
    Settings.llm = llm
    Settings.embed_model = embed_model
    
    # è®¾ç½®æ–‡æ¡£åˆ†å—å‚æ•°
    Settings.chunk_size = 1024
    Settings.chunk_overlap = 50
    
    print(f"âœ… æ¨¡å‹è®¾ç½®å®Œæˆ: LLM={llm_model}, åµŒå…¥={embed_model_name}")
    return llm, embed_model

def load_documents(data_path):
    """åŠ è½½æ–‡æ¡£"""
    from llama_index.core.readers import SimpleDirectoryReader
    from llama_index.readers.file import PDFReader
    from llama_index.core.readers.json import JSONReader
    
    print(f"ğŸ“ å¤„ç†ç›®å½•: {data_path}")
      # ä¸“ç”¨å¤„ç†å™¨
    pdf_reader = PDFReader()
    json_reader = JSONReader()
    
    reader = SimpleDirectoryReader(
        input_dir=data_path,
        file_extractor={
            ".pdf": pdf_reader,
            ".docx": "default", 
            ".txt": "default",
            ".json": json_reader
        },
        recursive=True
    )
    
    documents = reader.load_data()
    print(f"âœ… åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    return documents

def create_database_and_embeddings(data_path, db_path, collection_name="documents", force_recreate=False):
    """åˆ›å»ºæ•°æ®åº“å’Œç”ŸæˆåµŒå…¥å‘é‡"""
    print("ğŸ’¾ åˆ›å»ºæ•°æ®åº“å’ŒåµŒå…¥å‘é‡...")
    
    # å¦‚æœå¼ºåˆ¶é‡å»ºï¼Œåˆ é™¤ç°æœ‰æ•°æ®åº“
    if force_recreate:
        try:
            chroma_client.delete_collection(collection_name)
            print(f"ğŸ—‘ï¸  åˆ é™¤ç°æœ‰é›†åˆ: {collection_name}")
        except:
            pass
    
    # åˆ›å»ºæ•°æ®åº“ç›®å½•
    os.makedirs(db_path, exist_ok=True)
    chroma_client = chromadb.PersistentClient(path=db_path)
    
   
    
    # åˆ›å»ºæ–°é›†åˆ
    chroma_collection = chroma_client.get_or_create_collection(collection_name)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    
    # åŠ è½½æ–‡æ¡£
    documents = load_documents(data_path)
    if not documents:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£")
        return None
    
    # ç”Ÿæˆå‘é‡ç´¢å¼•
    print(f"ğŸ§® ä¸ºé›†åˆ {collection_name} ç”Ÿæˆå‘é‡ç´¢å¼•...")
    index = VectorStoreIndex.from_documents(
        documents,
        storage_context=storage_context,
        show_progress=True
    )
    
    final_count = chroma_collection.count()
    print(f"âœ… é›†åˆ {collection_name} åŒ…å« {final_count} ä¸ªå‘é‡")
    
    return index

def load_existing_database(db_path):
    """åŠ è½½ç°æœ‰æ•°æ®åº“"""
    print("ğŸ’¾ åŠ è½½ç°æœ‰æ•°æ®åº“...")
    
    if not os.path.exists(db_path):
        print("âŒ æ•°æ®åº“ä¸å­˜åœ¨")
        return None
    
    try:
        chroma_client = chromadb.PersistentClient(path=db_path)
        chroma_collection = chroma_client.get_collection("documents")
        existing_count = chroma_collection.count()
        
        if existing_count == 0:
            print("âš ï¸  æ•°æ®åº“ä¸ºç©º")
            return None
        
        print(f"âœ… åŠ è½½ç°æœ‰æ•°æ®åº“ï¼ŒåŒ…å« {existing_count} ä¸ªå‘é‡")
        
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        index = VectorStoreIndex.from_vector_store(vector_store)
        
        return index
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®åº“å¤±è´¥: {e}")
        return None

def test_queries(index, queries=None):
    """æµ‹è¯•æŸ¥è¯¢"""
    if queries is None:
        queries = [
            "who are you?",
            "give some information about the VL-4PQ Base",
            "When the logging tool passes through the bottom of the casing,  If peaks are too muted or if they consistently run off scale, what should we do?",
            ]
    
    print("ğŸ§ª æµ‹è¯•æŸ¥è¯¢...")
    query_engine = index.as_query_engine()
    
    for query in queries:
        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        try:
            response = query_engine.query(query)
            print(f"ğŸ’¡ å›ç­”: {response}")
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            print("ğŸ’¡ å¯èƒ½çš„åŸå› : LLMå“åº”è¶…æ—¶æˆ–OllamaæœåŠ¡é—®é¢˜")

def main():
    parser = argparse.ArgumentParser(description="åœ°è´¨æ•°æ®RAGç³»ç»Ÿ")
    parser.add_argument("--mode", choices=["create", "load", "auto"], default="auto",
                        help="è¿è¡Œæ¨¡å¼: create=å¼ºåˆ¶é‡æ–°åˆ›å»º, load=åªåŠ è½½ç°æœ‰, auto=è‡ªåŠ¨é€‰æ‹©")
    parser.add_argument("--data-path", default="/Users/yjli/QUTIT/semester4/ifn712/datacollect/cr088747-2014",
                        help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--db-path", default="./simple_geological_db",
                        help="æ•°æ®åº“è·¯å¾„")
    parser.add_argument("--llm-model", default="qwen2.5:7b",
                        help="LLMæ¨¡å‹åç§°")
    parser.add_argument("--embed-model", default="nomic-embed-text",
                        help="åµŒå…¥æ¨¡å‹åç§°")
    
    args = parser.parse_args()
    
    print("ğŸŒ åœ°è´¨æ•°æ®RAGç³»ç»Ÿ")
    print("="*50)
    print(f"ğŸ“‹ è¿è¡Œæ¨¡å¼: {args.mode}")
    print(f"ğŸ“ æ•°æ®è·¯å¾„: {args.data_path}")
    print(f"ğŸ’¾ æ•°æ®åº“è·¯å¾„: {args.db_path}")
    print("="*50)
    
    # è®¾ç½®æ¨¡å‹
    setup_models(args.llm_model, args.embed_model)
    
    index = None
    
    if args.mode == "create":
        # å¼ºåˆ¶é‡æ–°åˆ›å»º
        print("ğŸ”¨ å¼ºåˆ¶é‡æ–°åˆ›å»ºæ•°æ®åº“...")
        index = create_database_and_embeddings(args.data_path, args.db_path, force_recreate=True)
        
    elif args.mode == "load":
        # åªåŠ è½½ç°æœ‰æ•°æ®åº“
        print("ğŸ“– åªåŠ è½½ç°æœ‰æ•°æ®åº“...")
        index = load_existing_database(args.db_path)
        if index is None:
            print("âŒ æ— æ³•åŠ è½½ç°æœ‰æ•°æ®åº“ï¼Œè¯·ä½¿ç”¨ --mode create åˆ›å»ºæ–°æ•°æ®åº“")
            return
            
    else:  # auto mode
        # è‡ªåŠ¨é€‰æ‹©ï¼šå…ˆå°è¯•åŠ è½½ï¼Œå¤±è´¥åˆ™åˆ›å»º
        print("ğŸ¤– è‡ªåŠ¨æ¨¡å¼ï¼šå°è¯•åŠ è½½ç°æœ‰æ•°æ®åº“...")
        index = load_existing_database(args.db_path)
        
        if index is None:
            print("ğŸ’¡ æ²¡æœ‰å¯ç”¨çš„æ•°æ®åº“ï¼Œåˆ›å»ºæ–°æ•°æ®åº“...")
            index = create_database_and_embeddings(args.data_path, args.db_path)
    
    if index is None:
        print("âŒ æ— æ³•åˆ›å»ºæˆ–åŠ è½½æ•°æ®åº“")
        return
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries(index)
    
    print(f"\nâœ… å®Œæˆï¼æ•°æ®åº“ä½ç½®: {args.db_path}")

if __name__ == "__main__":
    main()