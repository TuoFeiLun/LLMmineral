# RAGçŸ¥è¯†é—®ç­”ç³»ç»Ÿ

åŸºäº **Qwen2.5 + LlamaIndex + Ollama** çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿï¼Œæ”¯æŒPDFã€Wordã€Markdownç­‰å¤šç§æ–‡æ¡£æ ¼å¼ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿ä½ çš„Macç³»ç»Ÿå·²å®‰è£…ï¼š
- Python 3.8+
- Ollama

### 2. å®‰è£…Ollamaå’Œæ¨¡å‹

```bash
# å®‰è£…Ollama (å¦‚æœæœªå®‰è£…)
brew install ollama

# å¯åŠ¨OllamaæœåŠ¡
ollama serve

# æ‹‰å–Qwen2.5æ¨¡å‹
ollama pull qwen2.5:7b
```

### 3. å®‰è£…Pythonä¾èµ–

```bash
# å…‹éš†æˆ–è¿›å…¥é¡¹ç›®ç›®å½•
cd /path/to/LLMmineral

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 4. ä½¿ç”¨æ–¹å¼

#### æ–¹å¼1ï¼šPythonä»£ç ä½¿ç”¨

```python
from rag.rag_system import RAGSystem

# åˆ›å»ºRAGç³»ç»Ÿ
rag = RAGSystem(model_name="qwen2.5:7b")

# æ·»åŠ æ–‡æ¡£
rag.add_documents("path/to/your/documents")

# æŸ¥è¯¢
result = rag.query("ä½ çš„é—®é¢˜")
print(result['answer'])
```

#### æ–¹å¼2ï¼šWebç•Œé¢

```bash
# å¯åŠ¨Streamlit Webç•Œé¢
streamlit run rag/streamlit_app.py
```

ç„¶ååœ¨æµè§ˆå™¨ä¸­è®¿é—® `http://localhost:8501`

#### æ–¹å¼3ï¼šå‘½ä»¤è¡Œç•Œé¢

```bash
# æ·»åŠ æ–‡æ¡£
python rag/cli_interface.py add "path/to/documents"

# æŸ¥è¯¢
python rag/cli_interface.py query "ä½ çš„é—®é¢˜"

# äº¤äº’å¼èŠå¤©
python rag/cli_interface.py chat --docs "path/to/documents"

# æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
python rag/cli_interface.py stats
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
LLMmineral/
â”œâ”€â”€ requirements.txt          # Pythonä¾èµ–
â”œâ”€â”€ README.md                # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ note.md                  # é¡¹ç›®ç¬”è®°
â””â”€â”€ rag/                     # RAGç³»ç»Ÿæ ¸å¿ƒä»£ç 
    â”œâ”€â”€ document_processor.py   # æ–‡æ¡£å¤„ç†å™¨
    â”œâ”€â”€ rag_system.py          # RAGç³»ç»Ÿä¸»ç¨‹åº
    â”œâ”€â”€ streamlit_app.py       # Webç•Œé¢
    â”œâ”€â”€ cli_interface.py       # å‘½ä»¤è¡Œç•Œé¢
    â””â”€â”€ example_usage.py       # ä½¿ç”¨ç¤ºä¾‹
```

## ğŸ¯ åŠŸèƒ½ç‰¹æ€§

### æ”¯æŒçš„æ–‡æ¡£æ ¼å¼
- âœ… PDF (.pdf)
- âœ… Wordæ–‡æ¡£ (.docx, .doc)
- âœ… Markdown (.md)
- âœ… çº¯æ–‡æœ¬ (.txt)

### æ ¸å¿ƒåŠŸèƒ½
- ğŸ” **æ™ºèƒ½æ£€ç´¢**: åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„è¯­ä¹‰æ£€ç´¢
- ğŸ’¬ **è‡ªç„¶å¯¹è¯**: æ”¯æŒä¸Šä¸‹æ–‡ç†è§£çš„é—®ç­”
- ğŸ“š **æ‰¹é‡å¤„ç†**: æ”¯æŒæ–‡ä»¶å¤¹æ‰¹é‡å¯¼å…¥æ–‡æ¡£
- ğŸ’¾ **æŒä¹…åŒ–å­˜å‚¨**: å‘é‡æ•°æ®åº“è‡ªåŠ¨ä¿å­˜ï¼Œé‡å¯åæ•°æ®ä¸ä¸¢å¤±
- ğŸŒ **å¤šç§ç•Œé¢**: Webç•Œé¢ã€å‘½ä»¤è¡Œç•Œé¢ã€Python API

### ç³»ç»Ÿé…ç½®
- **æ¨¡å‹**: Qwen2.5-7B (å¯é…ç½®)
- **å‘é‡æ•°æ®åº“**: ChromaDB
- **åˆ†å—å¤§å°**: 512 tokens (å¯é…ç½®)
- **æ£€ç´¢æ•°é‡**: Top-5 ç›¸ä¼¼æ–‡æ¡£ (å¯é…ç½®)

## ğŸ› ï¸ é«˜çº§é…ç½®

### è‡ªå®šä¹‰RAGç³»ç»Ÿ

```python
from rag.rag_system import RAGSystem

rag = RAGSystem(
    model_name="qwen2.5:7b",           # Ollamaæ¨¡å‹å
    embedding_model="qwen2.5:7b",      # åµŒå…¥æ¨¡å‹
    chunk_size=512,                    # æ–‡æ¡£åˆ†å—å¤§å°
    chunk_overlap=50,                  # åˆ†å—é‡å å¤§å°
    similarity_top_k=5,                # æ£€ç´¢æ–‡æ¡£æ•°é‡
    persist_dir="./custom_db"          # æ•°æ®åº“ç›®å½•
)
```

### ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```env
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=qwen2.5:7b
CHUNK_SIZE=512
SIMILARITY_TOP_K=5
```

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€æŸ¥è¯¢ç¤ºä¾‹

```python
from rag.rag_system import quick_setup

# å¿«é€Ÿè®¾ç½®å¹¶åŠ è½½æ–‡æ¡£
rag = quick_setup("documents/")

# æŸ¥è¯¢
result = rag.query("ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ")
print(f"å›ç­”: {result['answer']}")

# æŸ¥çœ‹ç›¸å…³æ–‡æ¡£
for source in result['sources']:
    print(f"æ–‡æ¡£: {source['metadata']['file_name']}")
    print(f"ç›¸ä¼¼åº¦: {source['score']:.3f}")
```

### æ‰¹é‡æ–‡æ¡£å¤„ç†

```python
# å¤„ç†æ•´ä¸ªæ–‡æ¡£ç›®å½•
rag.add_documents("./research_papers/")

# å¤šè½®å¯¹è¯
questions = [
    "è¿™äº›è®ºæ–‡çš„ä¸»è¦ç ”ç©¶é¢†åŸŸæ˜¯ä»€ä¹ˆï¼Ÿ",
    "æœ‰å“ªäº›é‡è¦çš„ç ”ç©¶æ–¹æ³•ï¼Ÿ",
    "å®éªŒç»“æœå¦‚ä½•ï¼Ÿ"
]

for q in questions:
    result = rag.query(q)
    print(f"Q: {q}")
    print(f"A: {result['answer']}\n")
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **Ollamaè¿æ¥å¤±è´¥**
   ```bash
   # æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€
   ollama list
   
   # é‡å¯OllamaæœåŠ¡
   ollama serve
   ```

2. **æ¨¡å‹æœªæ‰¾åˆ°**
   ```bash
   # ç¡®è®¤æ¨¡å‹å·²ä¸‹è½½
   ollama pull qwen2.5:7b
   ```

3. **æ–‡æ¡£åŠ è½½å¤±è´¥**
   - æ£€æŸ¥æ–‡æ¡£æ ¼å¼æ˜¯å¦æ”¯æŒ
   - ç¡®è®¤æ–‡ä»¶è·¯å¾„æ­£ç¡®
   - æŸ¥çœ‹é”™è¯¯æ—¥å¿—

4. **å†…å­˜ä¸è¶³**
   - å‡å° `chunk_size` å‚æ•°
   - å‡å°‘ `similarity_top_k` æ•°é‡
   - åˆ†æ‰¹å¤„ç†å¤§é‡æ–‡æ¡£

### æ€§èƒ½ä¼˜åŒ–

1. **æå‡æ£€ç´¢é€Ÿåº¦**
   ```python
   # å‡å°‘æ£€ç´¢æ–‡æ¡£æ•°é‡
   rag = RAGSystem(similarity_top_k=3)
   ```

2. **ä¼˜åŒ–å†…å­˜ä½¿ç”¨**
   ```python
   # ä½¿ç”¨è¾ƒå°çš„åˆ†å—å¤§å°
   rag = RAGSystem(chunk_size=256, chunk_overlap=25)
   ```

3. **æå‡å›ç­”è´¨é‡**
   ```python
   # å¢åŠ æ£€ç´¢æ–‡æ¡£æ•°é‡å’Œåˆ†å—é‡å 
   rag = RAGSystem(similarity_top_k=8, chunk_overlap=100)
   ```

## ğŸ“ å¼€å‘è¯´æ˜

### æ‰©å±•æ–‡æ¡£æ ¼å¼

åœ¨ `document_processor.py` ä¸­æ·»åŠ æ–°çš„æ–‡æ¡£å¤„ç†å™¨ï¼š

```python
def _load_custom_format(self, file_path: Path) -> List[Document]:
    """åŠ è½½è‡ªå®šä¹‰æ ¼å¼æ–‡ä»¶"""
    # å®ç°è‡ªå®šä¹‰æ ¼å¼è§£æé€»è¾‘
    pass
```

### è‡ªå®šä¹‰å‘é‡æ•°æ®åº“

```python
from llama_index.vector_stores.faiss import FaissVectorStore

# ä½¿ç”¨FAISSæ›¿ä»£ChromaDB
vector_store = FaissVectorStore(faiss_index)
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License

---

## ğŸ”— ç›¸å…³é“¾æ¥

- [Ollamaå®˜ç½‘](https://ollama.ai/)
- [LlamaIndexæ–‡æ¡£](https://docs.llamaindex.ai/)
- [Qwen2.5æ¨¡å‹](https://huggingface.co/Qwen/Qwen2.5-7B)
- [ChromaDBæ–‡æ¡£](https://docs.trychroma.com/)

---

"what is the content of the documents?",
 "give some information about the  VL -4PQ Base",
 "When the logging tool passes through the bottom of the casing,  If peaks are too muted or if they consistently run off scale, what should we do?"
 
